{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Here is an implementation of paper [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) on Brain Tumor dataset.\n* SRGAN uses the GAN to produce the high resolution images from the low resolution images. In this implementation, for training, the original 512 X 512 high resolution image is downsampled into low resolution image using Gaussian blur followed by resizing it to 128 X 128. A Generator is used to generate 512 X 512 images from 128 X 128 images and a discriminator is used to distinguish the generated images from the HR images."},{"metadata":{},"cell_type":"markdown","source":"### Let's create an output directory to store the images generated by the generator and also save the model after certain number of epochs"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.mkdir('./output')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here I am defining my GAN network architecture"},{"metadata":{},"cell_type":"markdown","source":"## GAN Network Architecture (Network.py)\n\n### The model is assembled from two components Discriminator and Generator. Discriminator - Responsible to distinguish between generated photos and real photos. Generator - Generate high resolution images from low resolution images."},{"metadata":{},"cell_type":"markdown","source":"![Network Image](https://github.com/mayank1101/Master-Thesis-Work/blob/main/Implementation%20Work/SRGAN/img/network.jpg?raw=true)"},{"metadata":{},"cell_type":"markdown","source":"* Above is the architecture of generator and discriminator used in the reference paper. k9n64s1 signifies kernel of size 9, 64 channels and stride of 1. Residual blocks are used in the discriminator. Two new concepts which are used in the network architecture are **PRelu** and **PixelShuffler**.\n* **PRelu:** is a kind of leakyRelu where instead of a predefined slope of 0.01, it makes it parameter for the neural network to itself decide the value of slope. y=ax when x<0 and y=x when x> 0 where a is the parameter to be determined by the network.\n* **PixelShuffling:** rearrange the tensor of shape (N,C,H,W) into (N,C/r*r,H*r,W*r) where r is the shuffling factor. It basically convert the depth(channel) into the space(height and width). In Generator, pixelshuffling is used to upsample the images size."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing all the necessary modules\n\nfrom keras.layers import Dense\nfrom keras.layers.core import Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import UpSampling2D\nfrom keras.layers.core import Flatten\nfrom keras.layers import Input\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.models import Model\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nfrom keras.layers import add","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Residual block\ndef res_block_gen(model, kernal_size, filters, strides):\n    \n    gen = model\n    \n    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n    # Using Parametric ReLU\n    model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n        \n    model = add([gen, model])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def up_sampling_block(model, kernal_size, filters, strides):\n    '''\n    In place of Conv2D and UpSampling2D we can also use Conv2DTranspose (Both are used for Deconvolution)\n    Even we can have our own function for deconvolution\n    model = Conv2DTranspose(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n    '''\n    \n    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n    model = UpSampling2D(size = 2)(model)\n    model = LeakyReLU(alpha = 0.2)(model)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator_block(model, filters, kernel_size, strides):\n    \n    model = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n    model = LeakyReLU(alpha = 0.2)(model)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generator\n\n* 7 Convolution blocks Each block with the same number of filters\n* PReLU with ( α = 0.2 ) is used as activation layer\n* 2 PixelShuffler layers for upsampling - PixelShuffler is feature map upscaling\n* Skip connections are used to achieve faster convergence"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Network Architecture is same as given in Paper https://arxiv.org/pdf/1609.04802.pdf\nclass Generator(object):\n\n    def __init__(self, noise_shape):\n        \n        self.noise_shape = noise_shape\n\n    def generator(self):\n        \n        gen_input = Input(shape = self.noise_shape)\n\n        model = Conv2D(filters = 64, kernel_size = 9, strides = 1, padding = \"same\")(gen_input)\n        model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n\n        gen_model = model\n\n        # Using 16 Residual Blocks\n        for index in range(16):\n            model = res_block_gen(model, 3, 64, 1)\n\n        model = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(model)\n        model = BatchNormalization(momentum = 0.5)(model)\n        model = add([gen_model, model])\n\n        # Using 2 UpSampling Blocks\n        for index in range(2):\n            model = up_sampling_block(model, 3, 256, 1)\n\n        model = Conv2D(filters = 1, kernel_size = 9, strides = 1, padding = \"same\")(model)\n        model = Activation('tanh')(model)\n\n        generator_model = Model(inputs = gen_input, outputs = model)\n\n        return generator_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discriminator\n\n* 16 Residual blocks Each block with increasing number of filters\n* LeakyReLU with ( α = 0.2 ) is used as activation layer\n* 2 Dense layers\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Network Architecture is same as given in Paper https://arxiv.org/pdf/1609.04802.pdf\nclass Discriminator(object):\n\n    def __init__(self, image_shape):\n        \n        self.image_shape = image_shape\n    \n    def discriminator(self):\n        \n        dis_input = Input(shape = self.image_shape)\n        \n        model = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(dis_input)\n        model = LeakyReLU(alpha = 0.2)(model)\n        \n        model = discriminator_block(model, 64, 3, 2)\n        model = discriminator_block(model, 128, 3, 1)\n        model = discriminator_block(model, 128, 3, 2)\n        model = discriminator_block(model, 256, 3, 1)\n        model = discriminator_block(model, 256, 3, 2)\n        model = discriminator_block(model, 512, 3, 1)\n        model = discriminator_block(model, 512, 3, 2)\n        \n        model = Flatten()(model)\n        model = Dense(1024)(model)\n        model = LeakyReLU(alpha = 0.2)(model)\n       \n        model = Dense(1)(model)\n        model = Activation('sigmoid')(model) \n        \n        discriminator_model = Model(inputs = dis_input, outputs = model)\n        return discriminator_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_gan_network(discriminator, shape, generator, optimizer):\n    discriminator.trainable = False\n    gan_input = Input(shape=shape)\n    x = generator(gan_input)\n    gan_output = discriminator(x)\n    gan = Model(inputs=gan_input, outputs=[x,gan_output])\n    gan.compile(loss=[\"binary_crossentropy\", \"binary_crossentropy\"],\n                loss_weights=[1., 1e-3],\n                optimizer=optimizer)\n\n    return gan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract the images from .mat file and preprocess them for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.applications.vgg19 import VGG19\nfrom keras.layers.convolutional import UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import SGD, Adam, RMSprop\nimport keras\nimport keras.backend as K\nfrom keras.layers import Lambda, Input\nimport tensorflow as tf\nimport skimage.transform\nfrom skimage import data, io, filters\nimport numpy as np\nfrom numpy import array\nfrom skimage.transform import rescale, resize\nfrom skimage.transform import resize\nimport os\n\n\nfrom matplotlib.pyplot import imread\nimport h5py\nimport cv2\n\nnp.random.seed(10)\nimage_shape = (512,512,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we have defined three custom functions load_path and load_data_from_dir and load_data which are used to read in the images from input directory. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_path(path):\n    directories = []\n    if os.path.isdir(path):\n        directories.append(path)\n    for elem in os.listdir(path): #check for nested dir within parent dir\n        if os.path.isdir(os.path.join(path,elem)):\n            directories = directories + load_path(os.path.join(path,elem))\n            directories.append(os.path.join(path,elem))\n    return directories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data_from_dirs(dirs, ext):\n    files = []\n    file_names = []\n    for d in dirs:\n        for f in os.listdir(d):\n            if f.endswith(ext):\n                image = h5py.File(os.path.join(d,f), 'r')\n                files.append(image)\n                file_names.append(os.path.join(d,f))\n    return files   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(directory, ext):\n\n    files = load_data_from_dirs(load_path(directory), ext)\n    return files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = load_data(\"../input/braintumor/BrainTumorData/\", \".mat\")\nx_train_h5 = files[:500]\nx_test_h5 = files[600:900]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now after reading the images, let's analyze them."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total files: ',len(files),' x_train: ', len(x_train_h5),' x_test', len(x_test_h5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train_h5[0], x_train_h5[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train_h5[0].keys())\nprint(x_train_h5[0]['cjdata'])\nprint(x_train_h5[0]['cjdata']['PID'])\nprint(x_train_h5[0]['cjdata']['image'])\nprint(x_train_h5[0]['cjdata']['label'])\nprint(x_train_h5[0]['cjdata']['tumorBorder'])\nprint(x_train_h5[0]['cjdata']['tumorMask'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Attributes and their meaning\n### Here we can clearly see that image has following attributes:\n* cjdata.label: 1 for meningioma, 2 for glioma, 3 for pituitary tumor\n* cjdata.PID: patient ID\n* cjdata.image: image data\n* cjdata.tumorBorder: a vector storing the coordinates of discrete points on the tumour border. For example, [x1, y1, x2, y2,…] in which x1, y1 are planar coordinates on tumour border. It was generated by manually delineating the tumour border. So we can use it to generate binary image of tumour mask.\n* cjdata.tumorMask: a binary image with 1s indicating tumour region"},{"metadata":{"trusted":true},"cell_type":"code","source":"## cjdata.PID: patient ID\n\nprint('Patient ID: ', files[0]['cjdata']['PID'][()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## cjdata.image: image data\n\nprint('Tumour Image Array: ', files[0]['cjdata']['image'][()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's visualize the image."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(files[0]['cjdata']['image'][()].squeeze(), cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## cjdata.label: 1 for meningioma, 2 for glioma, 3 for pituitary tumor\n\nprint('label: ', files[0]['cjdata']['label'][()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## cjdata.tumorBorder: a vector storing the coordinates of discrete points on the tumour border. \n## For example, [x1, y1, x2, y2,…] in which x1, y1 are planar coordinates on tumour border. \n## It was generated by manually delineating the tumour border. So we can use it to generate binary image of tumour mask.\n\nprint('Tumour Border: ', files[0]['cjdata']['tumorBorder'][()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## cjdata.tumorMask: a binary image with 1s indicating tumour region\n\nprint('Tumour Mask: ',files[0]['cjdata']['tumorMask'][()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's visualize the tumor mask"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(files[0]['cjdata']['tumorMask'][()].squeeze(), cmap='gray')\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we will do some preprocessing on the input images.\n* We will convert 512x512 to 512x512x1 to make it comaptible to model input.\n* Downsample each image by factor of 4.\n* Normalize all pixels between 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = [np.expand_dims(array(mat['cjdata']['image'][()]), axis=2) for mat in x_train_h5]\nx_test = [np.expand_dims(array(mat['cjdata']['image'][()]), axis=2) for mat in x_test_h5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(x_train),x_train[0].shape, type(x_test[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So, we can clearly see that we have 500 images each of size (512 X 512 X 1) and each image is a numpy array."},{"metadata":{},"cell_type":"markdown","source":"### Here we have defined few utility functions to achive our task of preprocessing the images.\n* hr_images convert image to numpy array.\n* lr_images downsample each image by factor of 4.\n* preprocess_HR scale pixel values between 0 and 1.\n* deprocess_HR scale up pixel values from 0-1 to 0-255.\n* preprocess_LR scale lower resolution image pixel values between 0 and 1.\n* deprocess_LRS clip the pixel values between 0-255. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def hr_images(images):\n    images_hr = array(images)\n    return images_hr\n\n\ndef lr_images(images_real , downscale):\n    \n    images = []\n    for img in  range(len(images_real)):\n        im = cv2.resize(images_real[img], \n                                 (images_real[img].shape[0]//downscale,images_real[img].shape[1]//downscale),\n                                 interpolation=cv2.INTER_CUBIC)\n        images.append(np.expand_dims(im,axis=2))\n    images_lr = array(images)\n    return images_lr\n\n\ndef preprocess_HR(x):\n    return np.divide(x.astype(np.float32), 127.5) - np.ones_like(x,dtype=np.float32)\n\n\ndef deprocess_HR(x):\n    input_data = (input_data + 1) * 127.5\n    return input_data.astype(np.uint8) \n\ndef preprocess_LR(x):\n    return np.divide(x.astype(np.float32), 255.)\n\ndef deprocess_LR(x):\n    x = np.clip(x*255, 0, 255)\n    return x\n\ndef normalize(input_data):\n    input_data_normalize = [(img.astype(np.float32) - 127.5)/127.5 for img in input_data]\n    return array(input_data_normalize)\n\ndef denormalize(input_data):\n    input_data = array([(img.astype(np.float32) + 1).astype(np.uint8) * 127.5 for img in input_data])\n    return input_data\n\ndef deprocess_LRS(x):\n    x = np.clip(x*255, 0, 255)\n    return x.astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess train dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_hr = hr_images(x_train)\nx_train_hr = normalize(x_train_hr)\nx_train_lr = lr_images(x_train, 4)\nx_train_lr = normalize(x_train_lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_hr = hr_images(x_test)\nx_test_hr = normalize(x_test_hr)\nx_test_lr = lr_images(x_test, 4)\nx_test_lr = normalize(x_test_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Now we will remove unwanted images form train set, beacuse we want to make sure that we have all the images of desired size 512x512."},{"metadata":{},"cell_type":"markdown","source":"### Preprocess train set images"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, img in enumerate(x_train_hr):\n    if img.shape != (512,512,1):\n        print(idx+1,' Image: ',img.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we can clearly see that we have two images of size (256,256,1) at indexes 155 and 480 which we don't want in training. Therefore we need to remove these images."},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, img in enumerate(x_train_lr):\n    if img.shape != (128,128,1):\n        print(idx+1,' Image: ',img.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we can clearly see that we have two images of size (64,64,1) at indexes 155 and 480 which we don't want in training. Therefore we need to remove these images."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_lr_new = []\nfor idx, img in enumerate(x_train_lr):\n    if img.shape == (128,128,1):\n        x_train_lr_new.append(img)\nx_train_lr_new = array(x_train_lr_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_lr_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After removing those unwanted images now we have 498 total images."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_hr_new = []\nfor i in range(len(x_train_hr)):\n    if x_train_hr[i].shape == (512,512,1):\n        x_train_hr_new.append(x_train_hr[i])\nx_train_hr_new = array(x_train_hr_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_hr_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After removing those unwanted images now we have 498 total images."},{"metadata":{},"cell_type":"markdown","source":"### Preprocess test images"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_test_hr.shape, x_test_hr[0].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We have total of 300 images in test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_hr_new = []\nfor i in range(len(x_test_hr)):\n    if x_test_hr[i].shape == (512,512,1):\n        x_test_hr_new.append(x_test_hr[i])\nx_test_hr_new = array(x_test_hr_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_hr_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_test_lr.shape, x_test_lr[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_lr_new = []\nfor i in range(len(x_test_lr)):\n    if x_test_lr[i].shape == (128,128,1):\n        x_test_lr_new.append(x_test_lr[i])\nx_test_lr_new = array(x_test_lr_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_lr_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After removing unwanted images from test set, now we have 287 images to test, all of same size."},{"metadata":{},"cell_type":"markdown","source":"## Now we will visualize few images from test set and see how they look after all the preprocessing work we did."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\n\nwidth=8\nheight=8\nrows = 3\ncols = 4\naxes=[]\n\nfig=plt.figure(figsize=(10,10))\n\nfor i in range(rows * cols):\n    axes.append( fig.add_subplot(rows, cols, i+1) )\n    subplot_title=(\"Image: \"+str(i))\n    axes[-1].set_title(subplot_title)  \n    plt.imshow(x_train[i].squeeze(), cmap='gray')\n    plt.axis('off')\nfig.tight_layout()    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation metrics\n* PSNR\n* Mean Squared Error\n* SSIM (Structure Similarity Index)\n* **Now we will define function compare_images which is used to evalute quality of generated results.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import mean_squared_error as mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_images(original, generated):\n    '''\n    Input:\n        original : Original HR Image\n        generated : Image Generated by GAN\n    return:\n        returns psnr and ssim metrics\n    '''\n    scores = []\n    scores.append(cv2.PSNR(original, generated))\n    scores.append(ssim(original, generated, multichannel=True))\n    scores.append(mse(original, generated))\n    \n    return scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we have defined a function plot_generated_images. What this function does is, after every 20 epochs, it evalute the generated images and compute metrics for each. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_generated_images(epoch,generator, examples=3 , dim=(1, 3), figsize=(15, 5)):\n    \n    rand_nums = np.random.randint(0, x_test_hr_new.shape[0], size=examples)\n    #print('rand_nums: ',rand_nums,' x_test_hr.shape[0]: ', x_test_hr.shape[0])\n    image_batch_hr = denormalize(x_test_hr_new[rand_nums])\n    image_batch_lr = np.stack(x_test_lr_new[rand_nums], axis=0)\n    gen_img = generator.predict(image_batch_lr)\n    generated_image = denormalize(gen_img)\n    image_batch_lr = denormalize(image_batch_lr)\n    \n    #generated_image = deprocess_HR(generator.predict(image_batch_lr))\n    if epoch % 20 == 0:\n        plt.figure(figsize=figsize)\n\n        plt.subplot(dim[0], dim[1], 1)\n        plt.imshow(image_batch_lr[1].squeeze(), interpolation='nearest')\n        plt.title('Down Sampled Image after eopch {}'.format(epoch))\n        plt.axis('off')\n\n        plt.subplot(dim[0], dim[1], 2)\n        plt.imshow(generated_image[1].squeeze(), interpolation='nearest')\n        plt.title('Image Generated by Generator of GAN eopch {}'.format(epoch))\n        plt.axis('off')\n\n        plt.subplot(dim[0], dim[1], 3)\n        plt.imshow(image_batch_hr[1].squeeze(), interpolation='nearest')\n        plt.title('Original HR Image eopch {}'.format(epoch))\n        plt.axis('off')\n\n        plt.tight_layout()\n        plt.savefig('./output/gan_generated_image_epoch_%d.png' % epoch)\n    \n    ## Printing PSNR AND SSIM\n    metric_score = compare_images(image_batch_hr[1], generated_image[1])\n    PSNR, SSIM, PMSE = metric_score[0], metric_score[1], metric_score[2]\n    return (PSNR, SSIM, PMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we will do the training"},{"metadata":{},"cell_type":"markdown","source":"### Here we define batch size and number of epochs to train our network."},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=220 # epochs = 20000\nbatch_size=4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we create our generator and discriminator objects and initialize our network"},{"metadata":{"trusted":true},"cell_type":"code","source":"downscale_factor = 4\n    \nbatch_count = int(x_train_hr_new.shape[0] / batch_size)\nshape = (image_shape[0]//downscale_factor, image_shape[1]//downscale_factor, image_shape[2])\ngenerator = Generator(shape).generator()\ndiscriminator = Discriminator(image_shape).discriminator()\n\nadam = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\ngenerator.compile(loss=\"binary_crossentropy\", optimizer=adam)\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=adam)\n\nshape = (image_shape[0]//downscale_factor, image_shape[1]//downscale_factor, 1)\ngan = get_gan_network(discriminator, shape, generator, adam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we do our training\n* Here, the LR images are passed into the generator to generate high resolution images and then pass these generated images as well as the original HR images to discriminator to get the fake label and real label respectively. Then, discriminator is trained that the generated images are fake images while the original HR images are real images by giving them labels as zeros and ones respectively.\n"},{"metadata":{},"cell_type":"markdown","source":"![Training Image](https://github.com/mayank1101/Master-Thesis-Work/blob/main/Implementation%20Work/SRGAN/img/architecture.jpg?raw=true)"},{"metadata":{"trusted":true},"cell_type":"code","source":"hr_loss, lr_loss, gan_loss = [], [], []\nm1, m2, m3 = [], [], []\n\nfor e in range(1, epochs+1):\n    print ('-'*15, 'Epoch %d' % e, '-'*15)\n    for i in range(batch_count):\n        rand_nums = np.random.randint(0, x_train_hr_new.shape[0], size=batch_size)\n\n        image_batch_hr =  np.stack(x_train_hr_new[rand_nums], axis=0)\n        image_batch_lr = np.stack(x_train_lr_new[rand_nums], axis=0)\n            \n        generated_images_sr = generator.predict(image_batch_lr)\n\n        real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n        fake_data_Y = np.random.random_sample(batch_size)*0.2\n\n        discriminator.trainable = True\n\n        d_loss_real = discriminator.train_on_batch(image_batch_hr, real_data_Y)\n        d_loss_fake = discriminator.train_on_batch(generated_images_sr, fake_data_Y)\n        #d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n\n        rand_nums = np.random.randint(0, x_train_hr_new.shape[0], size=batch_size)\n    \n        image_batch_hr =  np.stack(x_train_hr_new[rand_nums], axis=0)\n        image_batch_lr = np.stack(x_train_lr_new[rand_nums], axis=0)\n\n        gan_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n\n        discriminator.trainable = False\n        loss_gan = gan.train_on_batch(image_batch_lr, [image_batch_hr,gan_Y])\n\n    print(\"Loss HR , Loss LR, Loss GAN\")\n    print(d_loss_real, d_loss_fake, loss_gan)\n    hr_loss.append(d_loss_real)\n    lr_loss.append(d_loss_fake)\n    gan_loss.append(loss_gan)\n    \n    PSNR, SSIM, PMSE = plot_generated_images(e, generator)\n    m1.append(PSNR)\n    m2.append(SSIM)\n    m3.append(PMSE)\n#     plot_generated_images(e, generator)\n    \n    if e % 220 == 0:\n        generator.save('./output/gen_model%d.h5' % e)\n        discriminator.save('./output/dis_model%d.h5' % e)\n        gan.save('./output/gan_model%d.h5' % e)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we visualize our HR Image Loss, LR Image Loss and GAN Loss for each epoch\n### Let's understand what each loss means\n* **Loss Function:** Here we are using Perceptual loss. It comprises of Content(Reconstruction) loss and Adversarial loss.\n* **HR Loss** (Adversarial Loss) which is a binary_crossentropy\n* **LR Loss** (ContentLoss) which is a binary_crossentropy\n* **GAN Loss** (Perceptual Loss) Generator loss is actually sum of **Content Loss** and **Adversarial Loss**."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Now we will analyze our loss and evaluation metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing \n* (GAN Loss) Perceptual Loss\n* (HR Loss) Adversarial Loss\n* (LR Loss) Content(Reconstruction) loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = np.arange(1,len(hr_loss)+1)\n\nfig = plt.figure(figsize=(10,10))\nplt.subplot(3, 2, 1)\n\n## Plotting HR Loss\n\nplt.plot(epochs, hr_loss, label='hr_loss')\nplt.title('HR Image Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch #')\nplt.legend()\n\n## Plotting LR Loss\n\nplt.subplot(3,2,2)\nplt.plot(epochs, lr_loss, label='lr_loss', color='b')\nplt.title('LR Image Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch #')\nplt.legend() \n\n## Plotting GAN Loss\n\nloss, fun_1_loss, fun_3_loss = [], [], []\n\nfor l1, l2, l3 in gan_loss:\n    loss.append(l1)\n    fun_1_loss.append(l2)\n    fun_3_loss.append(l3)\n\nplt.subplot(3,2,3)\nplt.plot(epochs, loss, label='loss', color='g')\nplt.title('GAN Loss')\nplt.ylabel('loss')\nplt.xlabel('Epoch #')\nplt.legend() \n\n\n\n\nplt.subplot(3,2,4)\nplt.plot(epochs, fun_1_loss, label='fun_1_loss', color='y')\nplt.title('GAN Loss')\nplt.ylabel('functional_1_loss')\nplt.xlabel('Epoch #')\nplt.legend() \n\n\nplt.subplot(3,2,5)\nplt.plot(epochs, fun_3_loss, label='fun_3_loss', color='k')\nplt.title('GAN Loss')\nplt.ylabel('functional_3_loss')\nplt.xlabel('Epoch #')\nplt.legend() \n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize PSNR for each epoch"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs, m1, label='psnr', color='c')\nplt.title('PSNR for Each Epoch')\nplt.ylabel('PSNR')\nplt.xlabel('Epoch #')\nplt.legend() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize SSIM for each epoch"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs, m2, label='ssim', color='m')\nplt.title('SSIM for Each Epoch')\nplt.xlabel('HR/LR Image Loss')\nplt.ylabel('Epoch #')\nplt.legend() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize MSE for each epoch"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs, m3, label='mse')\nplt.title('MSE for Each Epoch')\nplt.ylabel('Mean Square Error')\nplt.xlabel('Epoch #')\nplt.legend() ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}